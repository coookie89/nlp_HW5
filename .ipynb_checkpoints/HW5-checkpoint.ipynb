{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines of Chinese:  5200757\n",
      "lines of English:  5200757\n",
      "-------- Get Corpus ! --------\n"
     ]
    }
   ],
   "source": [
    "#整理語料\n",
    "import json\n",
    "\n",
    "def get_coorpus(): #把json檔整理, 輸出一個中文檔 ＆ 一個英文檔\n",
    "    files = ['translation2019zh_train', 'translation2019zh_valid']\n",
    "    ch_path = './data/corpus.ch'\n",
    "    en_path = './data/corpus.en'\n",
    "    ch_lines = []\n",
    "    en_lines = []\n",
    "\n",
    "    for file in files:\n",
    "        with open ('./translation2019zh/' + file + '.json', 'r') as json_file:\n",
    "            for times, data in enumerate(json_file, 1):\n",
    "                item = json.loads(data)\n",
    "                ch_lines.append(item['chinese'] + '\\n')\n",
    "                en_lines.append(item['english'] + '\\n')\n",
    "                \n",
    "\n",
    "    with open(ch_path, \"w\") as fch:\n",
    "        fch.writelines(ch_lines)\n",
    "\n",
    "    with open(en_path, \"w\") as fen:\n",
    "        fen.writelines(en_lines)\n",
    "\n",
    "    # lines of Chinese: 252777\n",
    "    print(\"lines of Chinese: \", len(ch_lines))\n",
    "    # lines of English: 252777\n",
    "    print(\"lines of English: \", len(en_lines))\n",
    "    print(\"-------- Get Corpus ! --------\")\n",
    "    \n",
    "get_coorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立分詞模型\n",
    "import sentencepiece as spm\n",
    "\n",
    "def train(input_file, vocab_size, model_name, model_type, character_coverage):\n",
    "    \"\"\"\n",
    "    search on https://github.com/google/sentencepiece/blob/master/doc/options.md to learn more about the parameters\n",
    "    :param input_file: one-sentence-per-line raw corpus file. No need to run tokenizer, normalizer or preprocessor.\n",
    "                       By default, SentencePiece normalizes the input with Unicode NFKC.\n",
    "                       You can pass a comma-separated list of files.\n",
    "    :param vocab_size: vocabulary size, e.g., 8000, 16000, or 32000\n",
    "    :param model_name: output model name prefix. <model_name>.model and <model_name>.vocab are generated.\n",
    "    :param model_type: model type. Choose from unigram (default), bpe, char, or word.\n",
    "                       The input sentence must be pretokenized when using word type.\n",
    "    :param character_coverage: amount of characters covered by the model, good defaults are: 0.9995 for languages with\n",
    "                               rich character set like Japanse or Chinese and 1.0 for other languages with\n",
    "                               small character set.\n",
    "    \"\"\"\n",
    "    input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --character_coverage=%s ' \\\n",
    "                     '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 '\n",
    "    cmd = input_argument % (input_file, model_name, vocab_size, model_type, character_coverage)\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "\n",
    "def run():\n",
    "    en_input = './data/corpus.en'\n",
    "    en_vocab_size = 32000\n",
    "    en_model_name = 'eng'\n",
    "    en_model_type = 'bpe'\n",
    "    en_character_coverage = 1\n",
    "    train(en_input, en_vocab_size, en_model_name, en_model_type, en_character_coverage)\n",
    "\n",
    "    ch_input = './data/corpus.ch'\n",
    "    ch_vocab_size = 32000\n",
    "    ch_model_name = 'chn'\n",
    "    ch_model_type = 'bpe'\n",
    "    ch_character_coverage = 0.9995\n",
    "    train(ch_input, ch_vocab_size, ch_model_name, ch_model_type, ch_character_coverage)\n",
    "\n",
    "\n",
    "def test():\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    text = \"美国总统特朗普今日抵达夏威夷。\"\n",
    "\n",
    "    sp.Load(\"./chn.model\")\n",
    "    print(sp.EncodeAsPieces(text))\n",
    "    print(sp.EncodeAsIds(text))\n",
    "    a = [12907, 277, 7419, 7318, 18384, 28724]\n",
    "    print(sp.decode_ids(a))\n",
    "\n",
    "\n",
    "run()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
